{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APAlala:\n",
    "    def sec_paper(pagina_web, user_agent):\n",
    "        #Ingresando la dirección del diario y el user_agent del usuario\n",
    "        #devuelve las secciones con las que cuenta esa web\n",
    "        import urllib\n",
    "        from bs4 import BeautifulSoup\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from urllib.error import HTTPError\n",
    "        import re\n",
    "        arr_ppr = []\n",
    "        headers = {}\n",
    "        headers['User-Agent'] = user_agent\n",
    "        page = (pagina_web)\n",
    "        page_paper = urllib.request.Request(page, headers=headers)\n",
    "        page_paper_req = urllib.request.urlopen(page_paper).read().decode('utf-8')\n",
    "        soup_ppr = BeautifulSoup(page_paper_req, 'lxml')\n",
    "        ul_class_ppr=soup_ppr.find_all('ul')\n",
    "        for i in range(len(ul_class_ppr)):\n",
    "            dir_sppr = ul_class_ppr[i]\n",
    "            for link in dir_sppr.find_all('a'):\n",
    "                arr_ppr = np.append(arr_ppr, link.get('href'))\n",
    "    \n",
    "        arr_pp_app = []\n",
    "        arr_pp_s = pd.Series(arr_ppr)\n",
    "        arr_pp_s.dropna(inplace=True)\n",
    "        arr_pp_s.reset_index(drop=True, inplace=True)\n",
    "       \n",
    "        for i in range(len(arr_pp_s)):\n",
    "            arr_split = arr_pp_s[i].split(pagina_web)[-1]\n",
    "            arr_pp_app = np.append(arr_pp_app, arr_split)\n",
    "         \n",
    "        dir_tabl = pd.DataFrame({'Secciones' : arr_pp_app,\n",
    "                                 'Direcciones' : arr_pp_s})\n",
    "\n",
    "        indices = []\n",
    "        for i in range(len(dir_tabl['Secciones'])):\n",
    "            try:\n",
    "                if re.search(r'\\.ar$|.ar/$', dir_tabl['Secciones'].iloc[i]):\n",
    "                    indices = np.append(indices, i)\n",
    "                elif re.search(r'\\.com$|com/', dir_tabl['Secciones'].iloc[i]):\n",
    "                    indices = np.append(indices, i)\n",
    "                elif re.search(r'^http', dir_tabl['Secciones'].iloc[i]):\n",
    "                    indices = np.append(indices, i)\n",
    "                elif re.search(r'#$', dir_tabl['Secciones'].iloc[i]):\n",
    "                    indices = np.append(indices, i)\n",
    "                elif re.search(r'html$', dir_tabl['Secciones'].iloc[i]):\n",
    "                    indices = np.append(indices, i)\n",
    "                elif re.search(r'^/suscr', dir_tabl['Secciones'].iloc[i]):\n",
    "                    indices = np.append(indices, i)\n",
    "            except IndexError:\n",
    "                   pass\n",
    "            \n",
    "        dir_tabl.drop(indices, axis=0, inplace=True)\n",
    "        dir_tabl.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        return print('Las secciones del diario son:', dir_tabl['Secciones'].unique())\n",
    "         \n",
    "    def argie_tablyzer(pagina_web, seccion, user_agent):\n",
    "        #Genera una tabla con nombre de diario, sección, título, texto de la noticia,\n",
    "        #dificultad de lectura en número e interpretación, y fecha de acceso.\n",
    "        import urllib\n",
    "        from bs4 import BeautifulSoup\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from urllib.error import HTTPError\n",
    "        import re\n",
    "        from legibilidad import legibilidad\n",
    "        import datetime      \n",
    "        pagina_web = pagina_web\n",
    "        seccion = seccion\n",
    "        user_agent = user_agent\n",
    "        cant_art = 0\n",
    "        arr_ppr = []\n",
    "        headers = {}\n",
    "        headers['User-Agent'] = user_agent\n",
    "        page = (pagina_web)\n",
    "        if False:\n",
    "            try:\n",
    "                page_paper = urllib.request.Request(page, headers=headers)\n",
    "                page_paper_req = urllib.request.urlopen(page_paper).read().decode('utf-8')\n",
    "                soup_ppr = BeautifulSoup(page_paper_req, 'lxml')\n",
    "                ul_class_ppr=soup_ppr.find_all('ul')\n",
    "                dir_sppr = ul_class_ppr[0]\n",
    "                for link in dir_sppr.find_all('a'):\n",
    "                    arr_ppr = np.append(arr_ppr, link.get('href'))\n",
    "                arr_pp_app = []\n",
    "                arr_pp_s = pd.Series(arr_ppr)\n",
    "                arr_pp_s.dropna(inplace=True)\n",
    "                arr_pp_s.reset_index(drop=True, inplace=True)\n",
    "                for i in range(len(arr_pp_s)):\n",
    "                    arr_split = arr_pp_s[i].split('/')\n",
    "                    while '' in arr_split:\n",
    "                        arr_split.remove('')\n",
    "                    ultimo = arr_split[-1]\n",
    "                    arr_pp_app = np.append(arr_pp_app, ultimo)\n",
    "                dir_tabl = pd.DataFrame({'Secciones' : arr_pp_app,\n",
    "                                         'Direcciones' : arr_pp_s})\n",
    "            except IndexError:\n",
    "                    pass\n",
    "        \n",
    "        else:\n",
    "            page_paper = urllib.request.Request(page, headers=headers)\n",
    "            page_paper_req = urllib.request.urlopen(page_paper).read().decode('utf-8')\n",
    "            soup_ppr = BeautifulSoup(page_paper_req, 'lxml')\n",
    "            ul_class_ppr=soup_ppr.find_all('ul')\n",
    "            dir_sppr = ul_class_ppr[2]\n",
    "            for link in dir_sppr.find_all('a'):\n",
    "                arr_ppr = np.append(arr_ppr, link.get('href'))\n",
    "            arr_pp_app = []\n",
    "            arr_pp_s = pd.Series(arr_ppr)\n",
    "            arr_pp_s.dropna(inplace=True)\n",
    "            arr_pp_s.reset_index(drop=True, inplace=True)\n",
    "            for i in range(len(arr_pp_s)):\n",
    "                arr_split = arr_pp_s[i].split('/')\n",
    "                while '' in arr_split:\n",
    "                    arr_split.remove('')\n",
    "                ultimo = arr_split[-1]\n",
    "                arr_pp_app = np.append(arr_pp_app, ultimo)\n",
    "            dir_tabl = pd.DataFrame({'Secciones' : arr_pp_app,\n",
    "                                     'Direcciones' : arr_pp_s})\n",
    "  \n",
    "        sec = seccion\n",
    "        for i in range(len(dir_tabl['Direcciones'])):\n",
    "            if sec in dir_tabl['Secciones'][i]:\n",
    "                sec = dir_tabl['Direcciones'][i]\n",
    "                break\n",
    "\n",
    "        item = pd.Series([page]+[sec])\n",
    "        if item[0] not in item[1]:\n",
    "            res_web=(pd.Series([page]+[sec])).sum()\n",
    "        else:\n",
    "            res_web=item[1]    \n",
    "    \n",
    "        headers = {}\n",
    "        headers['User-Agent'] = user_agent\n",
    "        html = res_web\n",
    "        url = urllib.request.Request(html, headers=headers)\n",
    "        url = urllib.request.urlopen(url).read()\n",
    "        soup = BeautifulSoup(url, 'lxml')\n",
    "        dir_url = []\n",
    "        for i in soup.find_all('a'):\n",
    "            dir_url= np.append(dir_url, i.get('href'))\n",
    "        dir_url = pd.Series(dir_url)\n",
    "        paginas = pd.DataFrame({})\n",
    "        paginas['Direcciones'] = dir_url\n",
    "        paginas.dropna(inplace=True)\n",
    "        paginas.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "        urls=[]\n",
    "        for sec in np.arange(len(paginas['Direcciones'])):\n",
    "            urls = np.append(urls, paginas['Direcciones'].iloc[sec])\n",
    "    \n",
    "        urls = np.unique(urls)\n",
    "        urls_res = []\n",
    "        for i in range(len(urls)):\n",
    "            if re.search(r\"ar/+[0-9]\", urls[i]):\n",
    "                urls_res = np.append(urls_res, urls[i])\n",
    "            elif '/'+seccion+'/' == urls[i]:\n",
    "                np.delete(urls, [i])\n",
    "            elif '/'+seccion == urls[i]:\n",
    "                np.delete(urls, [i])\n",
    "            elif re.search(r\"^/[0-9]+-\", urls[i]):\n",
    "                urls_res = np.append(urls_res, urls[i])\n",
    "            elif re.search(r'^'+seccion, urls[i]):\n",
    "                urls_res = np.append(urls_res, urls[i])\n",
    "            elif re.search(r'^/'+seccion, urls[i]):\n",
    "                urls_res = np.append(urls_res, urls[i])\n",
    "    \n",
    "        todo_texto = []\n",
    "        diff_text= []\n",
    "        titulo_append = []\n",
    "        texto_append = []\n",
    "        interp = []\n",
    "        for i in np.arange(len(urls_res)):\n",
    "            try:\n",
    "                if pagina_web in urls_res[i]:\n",
    "                    busqueda = urls_res[i]\n",
    "                elif seccion == urls_res[i]:\n",
    "                    continue\n",
    "                else:\n",
    "                    busqueda = pagina_web+urls_res[i]\n",
    "                html_def = busqueda\n",
    "                url_def = urllib.request.Request(html_def, headers=headers)\n",
    "                url_def = urllib.request.urlopen(url_def).read()\n",
    "                soup_def = BeautifulSoup(url_def)\n",
    "                titulo_def = soup_def.find('title')\n",
    "                cant_art = cant_art +1\n",
    "                articulo_def = soup_def.find_all('p')\n",
    "                art_def_text=[re.sub(r'\\<.+?>|[\\n]|[\\r]',r'',str(a)) for a in articulo_def]\n",
    "                cuenta= []\n",
    "                for i in range(len(art_def_text)):\n",
    "                    cuenta = np.append(cuenta, len(art_def_text[i]))\n",
    "                defin = []\n",
    "                for i in np.arange(len(art_def_text)):\n",
    "                    if len(art_def_text[i])>(cuenta.mean()+40):\n",
    "                        defin= np.append(defin, art_def_text[i])\n",
    "                art_def_text = (pd.Series(defin)).sum()\n",
    "                art_def_text = art_def_text.replace('\\xa0', ' ')\n",
    "                if 'Recibir newsletter' in art_def_text:\n",
    "                    art_def_text = art_def_text.replace('Recibir newsletter', ' ')\n",
    "                ln_comentario = 'Los comentarios publicados son de exclusiva responsabilidad de sus autores y las consecuencias derivadas de ellos pueden ser pasibles de sanciones legales. Aquel usuario que incluya en sus mensajes algún comentario violatorio del reglamento será eliminado e inhabilitado para volver a comentar. Enviar un comentario implica la aceptación del Reglamento.'\n",
    "                if ln_comentario in art_def_text:\n",
    "                    art_def_text = art_def_text.replace(ln_comentario, '.')\n",
    "                if '.' in art_def_text:\n",
    "                    if '.' != '. ':\n",
    "                        art_def_text = art_def_text.replace('.', '. ')\n",
    "                titulo_def = re.sub(r'\\<title>|\\</title>|\\n', r'', str(titulo_def))\n",
    "                titulo_append = np.append(titulo_append, titulo_def)\n",
    "                texto_append = np.append(texto_append, str(art_def_text))\n",
    "                \n",
    "            except (AttributeError, HTTPError):\n",
    "                print('Imposible ejecutar sobre', busqueda, '\\n')\n",
    "                cant_art = cant_art - 1\n",
    "                continue\n",
    "            \n",
    "            seccion_final = re.sub(r'/category/asuntos-publicos/|/$|/secciones/|^/', r'', seccion)\n",
    "            diario = re.sub(r'^https://www\\.|\\.com|\\.ar$', r'', pagina_web)\n",
    "            diff_text = np.append(diff_text, legibilidad.szigriszt_pazos(art_def_text))\n",
    "            interp = np.append(interp, str(legibilidad.interpretaP(legibilidad.szigriszt_pazos(art_def_text))))\n",
    "             \n",
    "        return pd.DataFrame({'diario' : diario,'url' : pagina_web, 'seccion' : seccion_final, 'título' : titulo_append,'texto' : texto_append,'dificultad' : diff_text,'interpretacion de dif' : interp, 'fecha de acceso': datetime.datetime.now()})\n",
    "    \n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    \n",
    "    def que_palabras(texto):\n",
    "        #Devuelve las palabras más mencionadas en el texto\n",
    "        import nltk\n",
    "        import string\n",
    "        from collections import Counter\n",
    "        t_words = nltk.word_tokenize(texto.lower())\n",
    "        {word:True for word in t_words}\n",
    "        def build_bag_of_words_features(words):\n",
    "            return {word:True for word in words}\n",
    "        lista = ['¿' ,'¡', '\"\"', \"''\",'“', '”', '``', '-', '«', '»']\n",
    "        lista2 = ['así', 'si', 'sólo', 'modo', 'ción', 'unas', 'puede', 'menos', 'punto', 'caso', 'según', 'todas', 'ciento', 'vez', 'tal', 'condiciones©', 'pagina12', 'www', 'com', 'comment', 'clarín', 'clic', 'comentar', 'e-mail', '--']\n",
    "        useless_words = nltk.corpus.stopwords.words(\"spanish\") + list(string.punctuation) + list(string.digits) + lista + lista2\n",
    "        filtered_words = [word for word in t_words if not word in useless_words]\n",
    "        word_counter = Counter(filtered_words)\n",
    "        most_common_words = word_counter.most_common()[:]\n",
    "        \n",
    "        return most_common_words\n",
    "    \n",
    "    def guarda_con_eso(path_file, df):\n",
    "        #Guarda en el directorio especificado el archivo generado\n",
    "        from pathlib import Path\n",
    "        import os\n",
    "        import pandas as pd\n",
    "        path = Path(path_file)\n",
    "        archivos = os.listdir(path)\n",
    "        if len(archivos) == 0:\n",
    "            nombre_archivo = str(input('Ingrese nombre para archivo:'))\n",
    "            f = open(path/nombre_archivo, 'w+')\n",
    "            df.to_csv(path/nombre_archivo, index=False)\n",
    "        else:\n",
    "            print(archivos)\n",
    "            na = str(input('Seleccione archivo:'))\n",
    "            f = pd.read_csv(path/na)\n",
    "            f = pd.concat([f, df], axis=0)\n",
    "            f.reset_index(drop=True, inplace=True)\n",
    "            f.to_csv(path/na, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
