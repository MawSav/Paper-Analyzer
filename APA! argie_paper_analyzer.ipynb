{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diario_web(pagina_web, user_agent):\n",
    "    import urllib\n",
    "    from bs4 import BeautifulSoup\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    arr_ppr = []\n",
    "    headers = {}\n",
    "    headers['User-Agent'] = user_agent\n",
    "    page = (pagina_web)\n",
    "    page_paper = urllib.request.Request(page, headers=headers)\n",
    "    page_paper_req = urllib.request.urlopen(page_paper).read().decode('utf-8')\n",
    "    soup_ppr = BeautifulSoup(page_paper_req, 'html.parser')\n",
    "    ul_class_ppr=soup_ppr.find_all('ul')\n",
    "    for i in range(len(ul_class_ppr)):\n",
    "        dir_sppr = ul_class_ppr[i]\n",
    "        for link in dir_sppr.find_all('a'):\n",
    "            arr_ppr = np.append(arr_ppr, link.get('href'))\n",
    "    \n",
    "    arr_pp_app = []\n",
    "    arr_pp_s = pd.Series(arr_ppr)\n",
    "    arr_pp_s.dropna(inplace=True)\n",
    "    arr_pp_s.reset_index(drop=True, inplace=True)\n",
    "       \n",
    "    for i in range(len(arr_pp_s)):\n",
    "        arr_split = arr_pp_s[i].split(pagina_web)[-1]\n",
    "        arr_pp_app = np.append(arr_pp_app, arr_split)\n",
    "         \n",
    "    dir_tabl = pd.DataFrame({'Secciones' : arr_pp_app,\n",
    "                             'Direcciones' : arr_pp_s})\n",
    "\n",
    "    return print('Las secciones del diario son:', np.unique(arr_pp_app))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seleccion_diario(pagina_web, seccion, user_agent):\n",
    "    import urllib\n",
    "    from urllib.error import HTTPError\n",
    "    from bs4 import BeautifulSoup\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import re\n",
    "    from legibilidad import legibilidad\n",
    "    import nltk\n",
    "    import string\n",
    "    from collections import Counter\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    \n",
    "    pagina_web = pagina_web\n",
    "    seccion = seccion\n",
    "    user_agent = user_agent\n",
    "    cant_art = 0\n",
    "    arr_ppr = []\n",
    "    headers = {}\n",
    "    headers['User-Agent'] = user_agent\n",
    "    page = (pagina_web)\n",
    "    if False:\n",
    "        try:\n",
    "            page_paper = urllib.request.Request(page, headers=headers)\n",
    "            page_paper_req = urllib.request.urlopen(page_paper).read().decode('utf-8')\n",
    "            soup_ppr = BeautifulSoup(page_paper_req, 'html.parser')\n",
    "            ul_class_ppr=soup_ppr.find_all('ul')\n",
    "            dir_sppr = ul_class_ppr[0]\n",
    "            for link in dir_sppr.find_all('a'):\n",
    "                arr_ppr = np.append(arr_ppr, link.get('href'))\n",
    "            arr_pp_app = []\n",
    "            arr_pp_s = pd.Series(arr_ppr)\n",
    "            arr_pp_s.dropna(inplace=True)\n",
    "            arr_pp_s.reset_index(drop=True, inplace=True)\n",
    "            for i in range(len(arr_pp_s)):\n",
    "                arr_split = arr_pp_s[i].split('/')\n",
    "                while '' in arr_split:\n",
    "                    arr_split.remove('')\n",
    "                ultimo = arr_split[-1]\n",
    "                arr_pp_app = np.append(arr_pp_app, ultimo)\n",
    "            dir_tabl = pd.DataFrame({'Secciones' : arr_pp_app,\n",
    "                                     'Direcciones' : arr_pp_s})\n",
    "        except IndexError:\n",
    "                pass\n",
    "        \n",
    "    else:\n",
    "        page_paper = urllib.request.Request(page, headers=headers)\n",
    "        page_paper_req = urllib.request.urlopen(page_paper).read().decode('utf-8')\n",
    "        soup_ppr = BeautifulSoup(page_paper_req, 'html.parser')\n",
    "        ul_class_ppr=soup_ppr.find_all('ul')\n",
    "        dir_sppr = ul_class_ppr[2]\n",
    "        for link in dir_sppr.find_all('a'):\n",
    "            arr_ppr = np.append(arr_ppr, link.get('href'))\n",
    "        arr_pp_app = []\n",
    "        arr_pp_s = pd.Series(arr_ppr)\n",
    "        arr_pp_s.dropna(inplace=True)\n",
    "        arr_pp_s.reset_index(drop=True, inplace=True)\n",
    "        for i in range(len(arr_pp_s)):\n",
    "            arr_split = arr_pp_s[i].split('/')\n",
    "            while '' in arr_split:\n",
    "                arr_split.remove('')\n",
    "            ultimo = arr_split[-1]\n",
    "            arr_pp_app = np.append(arr_pp_app, ultimo)\n",
    "        dir_tabl = pd.DataFrame({'Secciones' : arr_pp_app,\n",
    "                                 'Direcciones' : arr_pp_s})\n",
    "  \n",
    "    sec = seccion\n",
    "    for i in range(len(dir_tabl['Direcciones'])):\n",
    "        if sec in dir_tabl['Secciones'][i]:\n",
    "            sec = dir_tabl['Direcciones'][i]\n",
    "            break\n",
    "\n",
    "    item = pd.Series([page]+[sec])\n",
    "    if item[0] not in item[1]:\n",
    "        res_web=(pd.Series([page]+[sec])).sum()\n",
    "    else:\n",
    "        res_web=item[1]    \n",
    "    \n",
    "    headers = {}\n",
    "    headers['User-Agent'] = user_agent\n",
    "    html = res_web\n",
    "    url = urllib.request.Request(html, headers=headers)\n",
    "    url = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(url, 'html.parser')\n",
    "    dir_url = []\n",
    "    for i in soup.find_all('a'):\n",
    "        dir_url= np.append(dir_url, i.get('href'))\n",
    "    dir_url = pd.Series(dir_url)\n",
    "    paginas = pd.DataFrame({})\n",
    "    paginas['Direcciones'] = dir_url\n",
    "    paginas.dropna(inplace=True)\n",
    "    paginas.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    urls=[]\n",
    "    for sec in np.arange(len(paginas['Direcciones'])):\n",
    "        urls = np.append(urls, paginas['Direcciones'].iloc[sec])\n",
    "    \n",
    "    urls = np.unique(urls)\n",
    "    urls_res = []\n",
    "    for i in range(len(urls)):\n",
    "        if re.search(r\"ar/+[0-9]\", urls[i]):\n",
    "            urls_res = np.append(urls_res, urls[i])\n",
    "        elif '/'+seccion+'/' == urls[i]:\n",
    "            np.delete(urls, [i])\n",
    "        elif '/'+seccion == urls[i]:\n",
    "            np.delete(urls, [i])\n",
    "        elif re.search(r\"^/[0-9]+-\", urls[i]):\n",
    "            urls_res = np.append(urls_res, urls[i])\n",
    "        elif re.search(r'^'+seccion, urls[i]):\n",
    "            urls_res = np.append(urls_res, urls[i])\n",
    "        elif re.search(r'^/'+seccion, urls[i]):\n",
    "            urls_res = np.append(urls_res, urls[i])\n",
    "    \n",
    "    todo_texto = []\n",
    "    diff_text= []\n",
    "    for i in np.arange(len(urls_res)):\n",
    "        try:\n",
    "            if pagina_web in urls_res[i]:\n",
    "                busqueda = urls_res[i]\n",
    "            elif seccion == urls_res[i]:\n",
    "                continue\n",
    "            else:\n",
    "                busqueda = pagina_web+urls_res[i]\n",
    "            html_def = busqueda\n",
    "            url_def = urllib.request.Request(html_def, headers=headers)\n",
    "            url_def = urllib.request.urlopen(url_def).read()\n",
    "            soup_def = BeautifulSoup(url_def)\n",
    "            titulo_def = soup_def.find('title')\n",
    "            cant_art = cant_art +1\n",
    "            articulo_def = soup_def.find_all('p')\n",
    "            art_def_text=[re.sub(r'\\<.+?>',r'',str(a)) for a in articulo_def]\n",
    "            cuenta= []\n",
    "            for i in range(len(art_def_text)):\n",
    "                cuenta = np.append(cuenta, len(art_def_text[i]))\n",
    "            defin = []\n",
    "            for i in np.arange(len(art_def_text)):\n",
    "                if len(art_def_text[i])>(cuenta.mean()+40):\n",
    "                    defin= np.append(defin, art_def_text[i])\n",
    "            art_def_text = (pd.Series(defin)).sum()\n",
    "            art_def_text = art_def_text.replace('\\xa0', ' ')\n",
    "            if 'Recibir newsletter' in art_def_text:\n",
    "                art_def_text = art_def_text.replace('Recibir newsletter', ' ')\n",
    "            ln_comentario = 'Los comentarios publicados son de exclusiva responsabilidad de sus autores y las consecuencias derivadas de ellos pueden ser pasibles de sanciones legales. Aquel usuario que incluya en sus mensajes algún comentario violatorio del reglamento será eliminado e inhabilitado para volver a comentar. Enviar un comentario implica la aceptación del Reglamento.'\n",
    "            if ln_comentario in art_def_text:\n",
    "                art_def_text = art_def_text.replace(ln_comentario, '.')\n",
    "            if '.' in art_def_text:\n",
    "                if '.' != '. ':\n",
    "                    art_def_text = art_def_text.replace('.', '. ')\n",
    "        except (AttributeError, HTTPError):\n",
    "            print('Imposible ejecutar sobre', busqueda, '\\n')\n",
    "            cant_art = cant_art - 1\n",
    "            continue\n",
    "            \n",
    "        todo_texto = np.append(todo_texto, art_def_text)\n",
    "        print('Título', titulo_def, '\\nDificultad de lectura (szigrizt_pazos):', legibilidad.interpretaP(legibilidad.szigriszt_pazos(art_def_text)), '\\n')\n",
    "        diff_text = np.append(diff_text, legibilidad.szigriszt_pazos(art_def_text))\n",
    "\n",
    "    print('\\nPromedio de dificultad de lectura:', legibilidad.interpretaP(np.average(diff_text)), '(',np.average(diff_text),')\\n\\n')\n",
    "    print('Cantidad de artículos analizados en', seccion,':', cant_art, '\\n')\n",
    "\n",
    "    todo_texto = (pd.Series(todo_texto)).sum()   \n",
    "    texto_words = nltk.word_tokenize(todo_texto.lower())\n",
    "    {word:True for word in texto_words}\n",
    "    def build_bag_of_words_features(words):\n",
    "        return {word:True for word in words}\n",
    "    lista = ['¿' ,'¡', '\"\"', \"''\",'“', '”', '``', '-', '«', '»']\n",
    "    lista2 = ['así', 'si', 'sólo', 'modo', 'ción', 'unas', 'puede', 'menos', 'punto', 'caso', 'según', 'todas', 'ciento', 'vez', 'tal', 'condiciones©', 'pagina12', 'www', 'com', 'comment', 'clarín', 'clic', 'comentar', 'e-mail', '--']\n",
    "    useless_words = nltk.corpus.stopwords.words(\"spanish\") + list(string.punctuation) + list(string.digits) + lista + lista2\n",
    "    filtered_words = [word for word in texto_words if not word in useless_words]\n",
    "    word_counter = Counter(filtered_words)\n",
    "    most_common_words = word_counter.most_common()[:]\n",
    "    \n",
    "    return print(most_common_words[:60])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
