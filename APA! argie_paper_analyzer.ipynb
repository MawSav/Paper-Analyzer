{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APAlala:\n",
    "    def sec_paper(pagina_web, user_agent):\n",
    "        import urllib\n",
    "        from bs4 import BeautifulSoup\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from urllib.error import HTTPError\n",
    "        arr_ppr = []\n",
    "        headers = {}\n",
    "        headers['User-Agent'] = user_agent\n",
    "        page = (pagina_web)\n",
    "        page_paper = urllib.request.Request(page, headers=headers)\n",
    "        page_paper_req = urllib.request.urlopen(page_paper).read().decode('utf-8')\n",
    "        soup_ppr = BeautifulSoup(page_paper_req, 'html.parser')\n",
    "        ul_class_ppr=soup_ppr.find_all('ul')\n",
    "        for i in range(len(ul_class_ppr)):\n",
    "            dir_sppr = ul_class_ppr[i]\n",
    "            for link in dir_sppr.find_all('a'):\n",
    "                arr_ppr = np.append(arr_ppr, link.get('href'))\n",
    "    \n",
    "        arr_pp_app = []\n",
    "        arr_pp_s = pd.Series(arr_ppr)\n",
    "        arr_pp_s.dropna(inplace=True)\n",
    "        arr_pp_s.reset_index(drop=True, inplace=True)\n",
    "       \n",
    "        for i in range(len(arr_pp_s)):\n",
    "            arr_split = arr_pp_s[i].split(pagina_web)[-1]\n",
    "            arr_pp_app = np.append(arr_pp_app, arr_split)\n",
    "         \n",
    "        dir_tabl = pd.DataFrame({'Secciones' : arr_pp_app,\n",
    "                                 'Direcciones' : arr_pp_s})\n",
    "\n",
    "        return print('Las secciones del diario son:', np.unique(arr_pp_app))\n",
    "    \n",
    "    def argie_panalyzer(pagina_web, seccion, user_agent):\n",
    "        import urllib\n",
    "        from bs4 import BeautifulSoup\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from urllib.error import HTTPError\n",
    "        import re\n",
    "        from legibilidad import legibilidad\n",
    "        import nltk\n",
    "        import string\n",
    "        from collections import Counter\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('stopwords')\n",
    "        \n",
    "        pagina_web = pagina_web\n",
    "        seccion = seccion\n",
    "        user_agent = user_agent\n",
    "        cant_art = 0\n",
    "        arr_ppr = []\n",
    "        headers = {}\n",
    "        headers['User-Agent'] = user_agent\n",
    "        page = (pagina_web)\n",
    "        if False:\n",
    "            try:\n",
    "                page_paper = urllib.request.Request(page, headers=headers)\n",
    "                page_paper_req = urllib.request.urlopen(page_paper).read().decode('utf-8')\n",
    "                soup_ppr = BeautifulSoup(page_paper_req, 'html.parser')\n",
    "                ul_class_ppr=soup_ppr.find_all('ul')\n",
    "                dir_sppr = ul_class_ppr[0]\n",
    "                for link in dir_sppr.find_all('a'):\n",
    "                    arr_ppr = np.append(arr_ppr, link.get('href'))\n",
    "                arr_pp_app = []\n",
    "                arr_pp_s = pd.Series(arr_ppr)\n",
    "                arr_pp_s.dropna(inplace=True)\n",
    "                arr_pp_s.reset_index(drop=True, inplace=True)\n",
    "                for i in range(len(arr_pp_s)):\n",
    "                    arr_split = arr_pp_s[i].split('/')\n",
    "                    while '' in arr_split:\n",
    "                        arr_split.remove('')\n",
    "                    ultimo = arr_split[-1]\n",
    "                    arr_pp_app = np.append(arr_pp_app, ultimo)\n",
    "                dir_tabl = pd.DataFrame({'Secciones' : arr_pp_app,\n",
    "                                         'Direcciones' : arr_pp_s})\n",
    "            except IndexError:\n",
    "                    pass\n",
    "        \n",
    "        else:\n",
    "            page_paper = urllib.request.Request(page, headers=headers)\n",
    "            page_paper_req = urllib.request.urlopen(page_paper).read().decode('utf-8')\n",
    "            soup_ppr = BeautifulSoup(page_paper_req, 'html.parser')\n",
    "            ul_class_ppr=soup_ppr.find_all('ul')\n",
    "            dir_sppr = ul_class_ppr[2]\n",
    "            for link in dir_sppr.find_all('a'):\n",
    "                arr_ppr = np.append(arr_ppr, link.get('href'))\n",
    "            arr_pp_app = []\n",
    "            arr_pp_s = pd.Series(arr_ppr)\n",
    "            arr_pp_s.dropna(inplace=True)\n",
    "            arr_pp_s.reset_index(drop=True, inplace=True)\n",
    "            for i in range(len(arr_pp_s)):\n",
    "                arr_split = arr_pp_s[i].split('/')\n",
    "                while '' in arr_split:\n",
    "                    arr_split.remove('')\n",
    "                ultimo = arr_split[-1]\n",
    "                arr_pp_app = np.append(arr_pp_app, ultimo)\n",
    "            dir_tabl = pd.DataFrame({'Secciones' : arr_pp_app,\n",
    "                                     'Direcciones' : arr_pp_s})\n",
    "  \n",
    "        sec = seccion\n",
    "        for i in range(len(dir_tabl['Direcciones'])):\n",
    "            if sec in dir_tabl['Secciones'][i]:\n",
    "                sec = dir_tabl['Direcciones'][i]\n",
    "                break\n",
    "\n",
    "        item = pd.Series([page]+[sec])\n",
    "        if item[0] not in item[1]:\n",
    "            res_web=(pd.Series([page]+[sec])).sum()\n",
    "        else:\n",
    "            res_web=item[1]    \n",
    "    \n",
    "        headers = {}\n",
    "        headers['User-Agent'] = user_agent\n",
    "        html = res_web\n",
    "        url = urllib.request.Request(html, headers=headers)\n",
    "        url = urllib.request.urlopen(url).read()\n",
    "        soup = BeautifulSoup(url, 'html.parser')\n",
    "        dir_url = []\n",
    "        for i in soup.find_all('a'):\n",
    "            dir_url= np.append(dir_url, i.get('href'))\n",
    "        dir_url = pd.Series(dir_url)\n",
    "        paginas = pd.DataFrame({})\n",
    "        paginas['Direcciones'] = dir_url\n",
    "        paginas.dropna(inplace=True)\n",
    "        paginas.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "        urls=[]\n",
    "        for sec in np.arange(len(paginas['Direcciones'])):\n",
    "            urls = np.append(urls, paginas['Direcciones'].iloc[sec])\n",
    "    \n",
    "        urls = np.unique(urls)\n",
    "        urls_res = []\n",
    "        for i in range(len(urls)):\n",
    "            if re.search(r\"ar/+[0-9]\", urls[i]):\n",
    "                urls_res = np.append(urls_res, urls[i])\n",
    "            elif '/'+seccion+'/' == urls[i]:\n",
    "                np.delete(urls, [i])\n",
    "            elif '/'+seccion == urls[i]:\n",
    "                np.delete(urls, [i])\n",
    "            elif re.search(r\"^/[0-9]+-\", urls[i]):\n",
    "                urls_res = np.append(urls_res, urls[i])\n",
    "            elif re.search(r'^'+seccion, urls[i]):\n",
    "                urls_res = np.append(urls_res, urls[i])\n",
    "            elif re.search(r'^/'+seccion, urls[i]):\n",
    "                urls_res = np.append(urls_res, urls[i])\n",
    "    \n",
    "        todo_texto = []\n",
    "        diff_text= []\n",
    "        for i in np.arange(len(urls_res)):\n",
    "            try:\n",
    "                if pagina_web in urls_res[i]:\n",
    "                    busqueda = urls_res[i]\n",
    "                elif seccion == urls_res[i]:\n",
    "                    continue\n",
    "                else:\n",
    "                    busqueda = pagina_web+urls_res[i]\n",
    "                html_def = busqueda\n",
    "                url_def = urllib.request.Request(html_def, headers=headers)\n",
    "                url_def = urllib.request.urlopen(url_def).read()\n",
    "                soup_def = BeautifulSoup(url_def)\n",
    "                titulo_def = soup_def.find('title')\n",
    "                cant_art = cant_art +1\n",
    "                articulo_def = soup_def.find_all('p')\n",
    "                art_def_text=[re.sub(r'\\<.+?>',r'',str(a)) for a in articulo_def]\n",
    "                cuenta= []\n",
    "                for i in range(len(art_def_text)):\n",
    "                    cuenta = np.append(cuenta, len(art_def_text[i]))\n",
    "                defin = []\n",
    "                for i in np.arange(len(art_def_text)):\n",
    "                    if len(art_def_text[i])>(cuenta.mean()+40):\n",
    "                        defin= np.append(defin, art_def_text[i])\n",
    "                art_def_text = (pd.Series(defin)).sum()\n",
    "                art_def_text = art_def_text.replace('\\xa0', ' ')\n",
    "                if 'Recibir newsletter' in art_def_text:\n",
    "                    art_def_text = art_def_text.replace('Recibir newsletter', ' ')\n",
    "                ln_comentario = 'Los comentarios publicados son de exclusiva responsabilidad de sus autores y las consecuencias derivadas de ellos pueden ser pasibles de sanciones legales. Aquel usuario que incluya en sus mensajes algún comentario violatorio del reglamento será eliminado e inhabilitado para volver a comentar. Enviar un comentario implica la aceptación del Reglamento.'\n",
    "                if ln_comentario in art_def_text:\n",
    "                    art_def_text = art_def_text.replace(ln_comentario, '.')\n",
    "                if '.' in art_def_text:\n",
    "                    if '.' != '. ':\n",
    "                        art_def_text = art_def_text.replace('.', '. ')\n",
    "            except (AttributeError, HTTPError):\n",
    "                print('Imposible ejecutar sobre', busqueda, '\\n')\n",
    "                cant_art = cant_art - 1\n",
    "                continue\n",
    "            \n",
    "            todo_texto = np.append(todo_texto, art_def_text)\n",
    "            print('Título', titulo_def, '\\nDificultad de lectura (szigrizt_pazos):', legibilidad.interpretaP(legibilidad.szigriszt_pazos(art_def_text)), '\\n')\n",
    "            diff_text = np.append(diff_text, legibilidad.szigriszt_pazos(art_def_text))\n",
    "\n",
    "        print('\\nPromedio de dificultad de lectura:', legibilidad.interpretaP(np.average(diff_text)), '(',np.average(diff_text),')\\n\\n')\n",
    "        print('Cantidad de artículos analizados en', seccion,':', cant_art, '\\n')\n",
    "\n",
    "        todo_texto = (pd.Series(todo_texto)).sum()   \n",
    "        texto_words = nltk.word_tokenize(todo_texto.lower())\n",
    "        {word:True for word in texto_words}\n",
    "        def build_bag_of_words_features(words):\n",
    "            return {word:True for word in words}\n",
    "        lista = ['¿' ,'¡', '\"\"', \"''\",'“', '”', '``', '-', '«', '»']\n",
    "        lista2 = ['así', 'si', 'sólo', 'modo', 'ción', 'unas', 'puede', 'menos', 'punto', 'caso', 'según', 'todas', 'ciento', 'vez', 'tal', 'condiciones©', 'pagina12', 'www', 'com', 'comment', 'clarín', 'clic', 'comentar', 'e-mail', '--']\n",
    "        useless_words = nltk.corpus.stopwords.words(\"spanish\") + list(string.punctuation) + list(string.digits) + lista + lista2\n",
    "        filtered_words = [word for word in texto_words if not word in useless_words]\n",
    "        word_counter = Counter(filtered_words)\n",
    "        most_common_words = word_counter.most_common()[:]\n",
    "    \n",
    "        return print(most_common_words[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las secciones del diario son: ['#' '/cartas_al_pais/' '/ciudades/' '/contactenos.html' '/deportes/'\n",
      " '/economia/' '/economia/divisas-acciones-bonos/' '/espectaculos/'\n",
      " '/mundo/' '/opinion/' '/policiales/' '/politica/' '/sociedad/' 'arq/'\n",
      " 'autos/' 'br/' 'buena-vida/' 'claringrilla/' 'clima/' 'cultura/'\n",
      " 'deportes/' 'deportes/agenda-deportiva/' 'deportes/ascenso/'\n",
      " 'deportes/basquet/liga-nacional/' 'deportes/estadisticas.html'\n",
      " 'deportes/futbol-internacional/' 'deportes/futbol/' 'deportes/hockey/'\n",
      " 'deportes/rugby/' 'deportes/seleccion-nacional/' 'deportes/tenis/'\n",
      " 'ediciones-anteriores/' 'entremujeres/' 'especiales-clarin/'\n",
      " 'espectaculos/' 'espectaculos/cine/' 'espectaculos/cine/cartelera.html'\n",
      " 'espectaculos/fama/' 'espectaculos/musica/' 'espectaculos/teatro/'\n",
      " 'espectaculos/teatro/cartelera.html' 'espectaculos/tv/' 'feriados/'\n",
      " 'fotogalerias' 'horoscopo.html' 'http://365.clarin.com'\n",
      " 'http://ar.cienradios.com/' 'http://autogestion.365.com.ar'\n",
      " 'http://comercial.agea.com.ar/' 'http://entremujeres.clarin.com'\n",
      " 'http://guia.clarin.com/' 'http://kiosco.clarin.com/'\n",
      " 'http://la100.cienradios.com/'\n",
      " 'http://qr.afip.gob.ar/?qr=qy9BfPZjufnExnl8h25PyA,,'\n",
      " 'http://radiomitre.cienradios.com/' 'http://tapas.clarin.com/'\n",
      " 'http://tn.com.ar/' 'http://www.argenprop.com/'\n",
      " 'http://www.ciudad.com.ar/' 'http://www.clarin.com/'\n",
      " 'http://www.clarin.com/contactenos.html'\n",
      " 'http://www.clarin.com/ediciones-anteriores'\n",
      " 'http://www.clarin.com/preguntas-frecuentes'\n",
      " 'http://www.clarin.com/rss.html' 'http://www.clasificados.clarin.com/'\n",
      " 'http://www.deautos.com.ar/' 'http://www.deautos.com/'\n",
      " 'http://www.eltrecetv.com.ar/' 'http://www.empleos.clarin.com/'\n",
      " 'http://www.empleos.clarin.com/postulantes/'\n",
      " 'http://www.grandt.clarin.com/' 'http://www.grupoclarin.com/'\n",
      " 'http://www.lavoz.com.ar/' 'http://www.losandes.com.ar/'\n",
      " 'http://www.ole.com.ar' 'http://www.rumbosdigital.com/'\n",
      " 'http://www.tycsports.com/' 'http://www.viapais.com.ar/'\n",
      " 'https://clasificados.clarin.com/'\n",
      " 'https://clasificados.clarin.com/inicio/index#!/'\n",
      " 'https://colecciones.clarin.com/' 'https://elle.clarin.com/'\n",
      " 'https://receptoriaonline.clarin.com/' 'https://twitter.com/clarincom'\n",
      " 'https://www.facebook.com/clarincom/' 'humor/' 'loterias-y-quinielas/'\n",
      " 'mundo/' 'new-york-times-international-weekly' 'newsletters/'\n",
      " 'noticias-mas-leidas/' 'policiales/' 'politica/' 'preguntas-frecuentes'\n",
      " 'radio-mitre-vivo/' 'revista-enie/' 'rural/' 'sociedad/' 'sudoku/'\n",
      " 'suscripciones/landing.html'\n",
      " 'suscripciones/landing.html?ns_campaign=clarin-footer&ns_mchannel=boton&ns_source=ageamkt&ns_linkname=clarin-footer&ns_fee=0.01'\n",
      " 'tecnologia/' 'television-argentina/' 'tema/aumento-de-tarifas.html'\n",
      " 'tema/los-cuadernos-de-las-coimas.html' 'tema/nba.html'\n",
      " 'tema/series_de_tv.html' 'tema/venezuela.html' 'temas/'\n",
      " 'ultimas-noticias/' 'viajes/' 'videos/' 'viva/']\n"
     ]
    }
   ],
   "source": [
    "APAlala.sec_paper('https://www.clarin.com/', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.96 Safari/537.36')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mauricio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mauricio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Título <title>La Ciudad colocó entre inversores privados un bono en pesos para cubrir las necesidades de 2019 - 13/02/2019 - Clarín.com</title> \n",
      "Dificultad de lectura (szigrizt_pazos): normal \n",
      "\n",
      "Título <title>Confirman multa contra un supermercado por no mostrar precios en góndola - 14/02/2019 - Clarín.com</title> \n",
      "Dificultad de lectura (szigrizt_pazos): bastante fácil \n",
      "\n",
      "Imposible ejecutar sobre https://www.clarin.com//economia/divisas-acciones-bonos/ \n",
      "\n",
      "Título <title>Dólar hoy: a cuánto cerró, banco por banco - 14/02/2019 - Clarín.com</title> \n",
      "Dificultad de lectura (szigrizt_pazos): fácil \n",
      "\n",
      "Título <title>Dólar hoy: a cuánto cerró, banco por banco - 13/02/2019 - Clarín.com</title> \n",
      "Dificultad de lectura (szigrizt_pazos): fácil \n",
      "\n",
      "Título <title>Dólar hoy: vuelve a subir y queda dentro de la banda cambiaria - 14/02/2019 - Clarín.com</title> \n",
      "Dificultad de lectura (szigrizt_pazos): bastante fácil \n",
      "\n",
      "Título <title>Tras el alza de la inflación suben el dólar, la tasa y el riesgo país - 15/02/2019 - Clarín.com</title> \n",
      "Dificultad de lectura (szigrizt_pazos): normal \n",
      "\n",
      "Título <title>En 2018 los créditos al sector privado cayeron 18% y creció la morosidad - 14/02/2019 - Clarín.com</title> \n",
      "Dificultad de lectura (szigrizt_pazos): normal \n",
      "\n",
      "Título <title>La Argentina le comprará menos gas a Bolivia y pagará con un avión Pampa si necesita más en invierno - 14/02/2019 - Clarín.com</title> \n",
      "Dificultad de lectura (szigrizt_pazos): normal \n",
      "\n",
      "Título <title>Argentina da un paso clave para aprobar el primer trigo transgénico del mundo - 13/02/2019 - Clarín.com</title> \n",
      "Dificultad de lectura (szigrizt_pazos): bastante difícil \n",
      "\n",
      "Título <title>Se derrumbó el financiamiento en el mercado de capitales - 15/02/2019 - Clarín.com</title> \n",
      "Dificultad de lectura (szigrizt_pazos): normal \n",
      "\n",
      "Título <title>Con foco en la recaudación impositiva el FMI se reunió con la AFIP - 15/02/2019 - Clarín.com</title> \n",
      "Dificultad de lectura (szigrizt_pazos): normal \n",
      "\n",
      "Título <title>Un fondo del lujo ingresa a la bodega de Susana Balbo - 14/02/2019 - Clarín.com</title> \n",
      "Dificultad de lectura (szigrizt_pazos): normal \n",
      "\n",
      "Título <title>Un gigante chino se asoció con Radio Victoria para fabricar TV y celulares - 15/02/2019 - Clarín.com</title> \n",
      "Dificultad de lectura (szigrizt_pazos): normal \n",
      "\n",
      "Título <title>El Gobierno impulsa un proyecto para que el Banco Central deje de financiar al Tesoro - 14/02/2019 - Clarín.com</title> \n",
      "Dificultad de lectura (szigrizt_pazos): normal \n",
      "\n",
      "Título <title>El Gobierno anuncia medidas para favorecer a las economías regionales - 13/02/2019 - Clarín.com</title> \n",
      "Dificultad de lectura (szigrizt_pazos): normal \n",
      "\n",
      "Título <title>El Gobierno rechazará el recurso de Tecpetrol - 15/02/2019 - Clarín.com</title> \n",
      "Dificultad de lectura (szigrizt_pazos): normal \n",
      "\n",
      "Título <title>Nokia retorna al país y Huawei busca un socio - 15/02/2019 - Clarín.com</title> \n",
      "Dificultad de lectura (szigrizt_pazos): bastante fácil \n",
      "\n",
      "Título <title>Cómo quedarán las jubilaciones y la AUH tras los aumentos de marzo y junio - 13/02/2019 - Clarín.com</title> \n",
      "Dificultad de lectura (szigrizt_pazos): bastante fácil \n",
      "\n",
      "Título <title>La recuperación del consumo llegaría recién hacia fin de año - 14/02/2019 - Clarín.com</title> \n",
      "Dificultad de lectura (szigrizt_pazos): bastante difícil \n",
      "\n",
      "Título <title>El secretario de Energía estimó que el gas no subiría más de 30% este año - 15/02/2019 - Clarín.com</title> \n",
      "Dificultad de lectura (szigrizt_pazos): normal \n",
      "\n",
      "Título <title>La tasa de las Leliq cae a 43,97% y el Central desacelera la baja - 13/02/2019 - Clarín.com</title> \n",
      "Dificultad de lectura (szigrizt_pazos): fácil \n",
      "\n",
      "Título <title>La vivienda, cada vez más lejos del sueldo - 13/02/2019 - Clarín.com</title> \n",
      "Dificultad de lectura (szigrizt_pazos): bastante fácil \n",
      "\n",
      "Título <title>Economías regionales: ya rige la rebaja de aportes patronales - 15/02/2019 - Clarín.com</title> \n",
      "Dificultad de lectura (szigrizt_pazos): normal \n",
      "\n",
      "Título <title>Un fondo árabe invierte US$ 55 millones en la petrolera de Miguel Galuccio - 13/02/2019 - Clarín.com</title> \n",
      "Dificultad de lectura (szigrizt_pazos): bastante fácil \n",
      "\n",
      "Título <title>Impuestos: AFIP garantizó a las pymes un techo a las tasas del 3% - 14/02/2019 - Clarín.com</title> \n",
      "Dificultad de lectura (szigrizt_pazos): normal \n",
      "\n",
      "Título <title>La inflación de enero fue del 2,9% y se mantiene alta por las subas en los alimentos - 14/02/2019 - Clarín.com</title> \n",
      "Dificultad de lectura (szigrizt_pazos): fácil \n",
      "\n",
      "Título <title>La inflación de 2,9% de enero: rubro por rubro, cuáles fueron los aumentos - 14/02/2019 - Clarín.com</title> \n",
      "Dificultad de lectura (szigrizt_pazos): fácil \n",
      "\n",
      "Título <title>Este mes sube el transporte, en marzo la luz y en abril, el gas - 14/02/2019 - Clarín.com</title> \n",
      "Dificultad de lectura (szigrizt_pazos): bastante fácil \n",
      "\n",
      "Título <title>Propiedades: en enero se necesitaron cuatro salarios para comprar un m2 - 13/02/2019 - Clarín.com</title> \n",
      "Dificultad de lectura (szigrizt_pazos): bastante fácil \n",
      "\n",
      "Título <title>Provincia: en diciembre crecieron por segundo mes las exportaciones - 15/02/2019 - Clarín.com</title> \n",
      "Dificultad de lectura (szigrizt_pazos): normal \n",
      "\n",
      "Título <title>¿Salen realmente $ 1 los vuelos que ofrecen las low cost? - 14/02/2019 - Clarín.com</title> \n",
      "Dificultad de lectura (szigrizt_pazos): bastante fácil \n",
      "\n",
      "\n",
      "Promedio de dificultad de lectura: normal ( 63.80096774193549 )\n",
      "\n",
      "\n",
      "Cantidad de artículos analizados en /economia/ : 30 \n",
      "\n",
      "[('millones', 66), ('año', 58), ('us', 40), ('precios', 33), ('central', 33), ('gas', 32), ('mercado', 31), ('inflación', 28), ('argentina', 28), ('pasado', 26), ('enero', 24), ('dólar', 22), ('tasa', 22), ('2018', 21), ('promedio', 21), ('000', 20), ('acuerdo', 19), ('meses', 19), ('aumento', 19), ('mayor', 18), ('argentino', 18), ('empresa', 18), ('banco', 18), ('hoy', 18), ('buenos', 18), ('puntos', 17), ('interanual', 17), ('gobierno', 17), ('pesos', 16), ('total', 16), ('cuenta', 16), ('parte', 16), ('mes', 16), ('diciembre', 16), ('años', 16), ('salarios', 16), ('valores', 15), ('respecto', 15), ('precio', 15), ('fmi', 15), ('hace', 14), ('baja', 14), ('producción', 14), ('registro', 13), ('suba', 13), ('aunque', 13), ('caída', 13), ('sector', 13), ('marzo', 13), ('local', 12), ('enviar', 12), ('propiedad', 12), ('dos', 12), ('país', 12), ('nuevos', 12), ('m2', 12), ('ciudad', 11), ('debés', 11), ('activar', 11), ('haciendo', 11)]\n"
     ]
    }
   ],
   "source": [
    "APAlala.argie_panalyzer('https://www.clarin.com/','/economia/', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.96 Safari/537.36')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
